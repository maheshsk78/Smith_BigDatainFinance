{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Big Data using Pyspark.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxHbL2ikkyO0",
        "outputId": "ba682a9a-0b63-4b96-bdd3-d58441d52ac8"
      },
      "source": [
        "pip install pyspark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 57kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 45.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=6c90cc93db96423bede81e4a9105ff80e4b68e5d188a0403aa011113c0caac5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvuOjxjkkow9"
      },
      "source": [
        "from pyspark import SparkContext\n",
        "sc =SparkContext()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AregD2x75ow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e009445f-0f07-481b-b31f-1e4d215447af"
      },
      "source": [
        "# Print the version of SparkContext\n",
        "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
        "\n",
        "# Print the Python version of SparkContext\n",
        "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
        "\n",
        "# Print the master of SparkContext\n",
        "print(\"The master of Spark Context in the PySpark shell is\", sc.master)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The version of Spark Context in the PySpark shell is 3.0.1\n",
            "The Python version of Spark Context in the PySpark shell is 3.6\n",
            "The master of Spark Context in the PySpark shell is local[*]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnaelJnQ8FsH"
      },
      "source": [
        "# Create a python list of numbers from 1 to 100 \n",
        "numb = range(1, 100)\n",
        "\n",
        "# Load the list into PySpark  \n",
        "spark_data = sc.parallelize(numb)\n",
        "\n",
        "# Load a local file into PySpark shell\n",
        "lines = sc.textFile(\"file:///usr/local/spark/README.md\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAddf3lV9n3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3a4cf4b-05b5-4370-d1c2-1ecd6a915259"
      },
      "source": [
        "#\n",
        "items = [1,2,3]\n",
        "list(map(lambda x: x*5,items))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5, 10, 15]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wCtZAT7bWTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2378eb73-d9d5-4e2b-b96d-6546cb25f6ed"
      },
      "source": [
        "# Create PairRDD Rdd with key value pairs\n",
        "Rdd = sc.parallelize([(1,2),(3,4),(3,6),(4,5)])\n",
        "\n",
        "# Apply reduceByKey() operation on Rdd\n",
        "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x+y)\n",
        "\n",
        "# Iterate over the result and print the output\n",
        "for num in Rdd_Reduced.collect(): \n",
        "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
        "\n",
        "# Sort the reduced RDD with the key by descending order\n",
        "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\n",
        "\n",
        "# Iterate over the result and print the output\n",
        "for num in Rdd_Reduced_Sort.collect():\n",
        "  print(\"Key {} has {} Counts\".format(num[0], num[1]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Key 4 has 5 Counts\n",
            "Key 1 has 2 Counts\n",
            "Key 3 has 10 Counts\n",
            "Key 4 has 5 Counts\n",
            "Key 3 has 10 Counts\n",
            "Key 1 has 2 Counts\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}